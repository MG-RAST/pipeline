#!/usr/bin/env perl 

use strict;
use warnings;
no warnings('once');

use Pipeline;
use Pipeline_Conf;

use JSON;
use Data::Dumper;
use Mail::Mailer;
use Getopt::Long;
use File::Copy;
use LWP::UserAgent;
use HTTP::Request::StreamingUpload;
use DateTime;
umask 000;

my $stage_name = "done";
my $stage_id = "999";
my $revision = "0";
my $version  = $Pipeline_Conf::pipeline_version . "." . $revision;
my $runcmd   = "seq_length_stats.py";
my $mg_email = 'Metagenomics Analysis Server <mg-rast@mcs.anl.gov>';
my $mg_link  = 'http://metagenomics.anl.gov/metagenomics.cgi?page=MetagenomeOverview&metagenome=';

# options
my $job_num  = "";
my $raw_fna  = "";
my $ver_db   = $Pipeline_Conf::ach_annotation_ver;
my $no_stats = 0;
my $no_file  = 0;
my $email    = 0;
my $ver      = "";
my $help     = "";
my $options  = GetOptions ("job=i"     => \$job_num,
			   "raw_fna:s" => \$raw_fna,
			   "ver_db:s"  => \$ver_db,
			   "no_stats!" => \$no_stats,
			   "no_file!"  => \$no_file,
			   "email!"    => \$email,
			   "version!"  => \$ver,
			   "help!"     => \$help,
			 );

if ( $ver ) {
  print STDERR "$stage_name - $version\n";
  exit(0);
} elsif ( $help or (! $job_num) ) {
  print STDERR "Usage: pipeline_done -j <job number> [-r <raw fasta> --ver_db <db version> --email --no_stats --no_file]\n";
  exit(1);
} 

my $log = Pipeline::logger($job_num);
$log->info("Starting $stage_name on job: $job_num");

# gunzip files
system("compress_job -j $job_num -u") == 0 or fail($log, "uncompress_job: $stage_name failed on job: $job_num");

if ($no_stats) {
  my $result_files = Pipeline::get_result_files();
  push @$result_files, "999.done.domain.stats";
  foreach my $rf (@$result_files) {
    my $file_path = $Pipeline_Conf::global_job_dir."/".$job_num."/".$Pipeline_Conf::results_dir."/".$rf;
    if (! -f $file_path) {
      $log->error("$stage_name failed on job: $job_num, missing file: $file_path");
      exit(1);
    }
  }
  $log->info("Finished $stage_name on job: $job_num, file check only");
  exit(0);
}

# update jobcache stage status
Pipeline::update_stage_info($job_num, $stage_name, "running");

my $job_dir     = $Pipeline_Conf::global_job_dir."/".$job_num;
my $proc_dir    = $job_dir."/proc";
my $run_dir     = $proc_dir."/".$stage_id.".".$stage_name;
my $results_dir = $job_dir."/".$Pipeline_Conf::results_dir;
my $hostname    = `hostname`;
chomp $hostname;

# create directories
if (-d $run_dir) {
  my $t = time;
  $log->info("found $run_dir, moving to $run_dir.$t");
  move($run_dir, $run_dir.".".$t) or fail($log, "$!");
}
mkdir($run_dir) or fail($log, "Could not mkdir: $run_dir, $!");
$log->info("Running on host $hostname, using dir $run_dir");

my $out_file = "$run_dir/$runcmd.out";
my $message  = "$stage_name failed on job: $job_num, see $out_file for details.";

# store drisee score
my $d_stats  = "$results_dir/075.drisee.stats";
my $d_score = 0;
if (-s $d_stats) {
  $d_score = `head -2 $d_stats | tail -1 | cut -f8`;
  chomp $d_score;
}
my $res = Pipeline::set_job_statistics($job_num, [["drisee_score_raw", sprintf("%.3f", $d_score)]]);
unless ($res) {
  $log->info("loaded drisee_score_raw stat: $d_score");
}

# load percentage of total reads for which a coverage score was read from seq header
my $cov_sum = "$results_dir/700.annotation.coverage.summary";
if (-s $cov_sum) {
  my $cov_per = 0;
  $cov_per = `head -1 $cov_sum | cut -f2`;
  chomp $cov_per;
  my $stat_res = Pipeline::set_job_statistics($job_num, [["percent_reads_with_coverage", $cov_per]]);
  unless ($stat_res) {
    $log->info("loaded percent_reads_with_coverage stat: $cov_per");
  }
}

my (@fna_files, @faa_files);
my $result_files = Pipeline::get_result_files();
my $stat_tags    = Pipeline::get_job_stat_tags();

# check if result files exist
foreach my $rf (@$result_files) {
  if ((! -f "$results_dir/$rf") && (! $no_file)) {
    fail($log, "$stage_name failed on job: $job_num, missing file: $results_dir/$rf");
  }
  if ((-s "$results_dir/$rf") && ($rf =~ /\.fna$/)) { push @fna_files, $rf; }
  if ((-s "$results_dir/$rf") && ($rf =~ /\.faa$/)) { push @faa_files, $rf; }
}

# run stats on dna fasta files
foreach my $fna (@fna_files) {
  system(qq(echo "$runcmd on $fna .." >> $out_file));
  if ($fna eq $Pipeline::preprocessed_fasta) {
    system("$runcmd -t fasta -i $results_dir/$fna -o $run_dir/$fna.stats -l $run_dir/$fna.lens -g $run_dir/$fna.gcs >> $out_file 2>&1") == 0 or fail($log, $message);
  } else {
    system("$runcmd -t fasta -i $results_dir/$fna -o $run_dir/$fna.stats >> $out_file 2>&1") == 0 or fail($log, $message);
  }
}
  
# run stats on aa fasta files
foreach my $faa (@faa_files) {
  system(qq(echo "$runcmd on $faa .." >> $out_file));
  system("$runcmd -f -t fasta -i $results_dir/$faa -o $run_dir/$faa.stats >> $out_file 2>&1") == 0 or fail($log, $message);
}

# run stats on cluster files
foreach my $clust (($Pipeline::cluster_map_rna, $Pipeline::cluster_map_aa)) {
  unless (-s "$results_dir/$clust") { next; }
  my $c_num = 0;
  my $c_seq = 0;
  
  open(CLUST, "<$results_dir/$clust") or fail($log, "$stage_name failed on job: $job_num, can not open $results_dir/$clust: $!");
  while (my $line = <CLUST>) {
    chomp $line;
    my @tabs = split(/\t/, $line);
    $c_seq += scalar( split(/,/, $tabs[2]) ) + 1;
    $c_num += 1;
  }
  close CLUST;

  open(CSTAT, ">$run_dir/$clust.stats") or fail($log, "$stage_name failed on job: $job_num, can not open $run_dir/$clust.stats: $!");
  print CSTAT "cluster_count\t$c_num\nclustered_sequence_count\t$c_seq\n";
  close CSTAT;
}

if ($raw_fna && (-s $raw_fna)) {
  my $format    = ($raw_fna =~ /\.(fq|fastq)$/) ? 'fastq' : 'fasta';
  my @raw_tags  = grep { $_ =~ /_raw$/ } @$stat_tags;
  my $raw_size  = -s $raw_fna;
  #my ($raw_md5) = (`md5sum $raw_fna` =~ /^(\S+)/);
  
  system(qq(echo "$runcmd on $raw_fna .." >> $out_file));
  system("$runcmd -t $format -i $raw_fna -o $raw_fna.stats -l $raw_fna.lens -g $raw_fna.gcs >> $out_file 2>&1") == 0 or fail($log, $message);

  load_fasta_stats($job_num, $raw_fna, "$raw_fna.stats", \@raw_tags, "raw");
  Pipeline::set_jobcache_info($job_num, "file_size_raw", $raw_size);
  #Pipeline::set_jobcache_info($job_num, "file_checksum_raw", $raw_md5);
}

# load derep stats
my $derep_rm_file = $run_dir."/".$Pipeline::dereplication_rm_fasta.'.stats';
my $derep_rm_seq  = '0';
if ((-s $derep_rm_file) && (-s $results_dir."/".$Pipeline::dereplication_rm_fasta)) {
    $derep_rm_seq = `grep -w sequence_count $derep_rm_file | cut -f2`;
    chomp $derep_rm_seq;
}
load_stats($job_num, [['sequence_count_dereplication_removed', $derep_rm_seq]]);

# load preprocessed stats
my @pre_rna_tags = grep { $_ =~ /_preprocessed_rna$/ } @$stat_tags;
load_fasta_stats($job_num, $results_dir."/".$Pipeline::preprocessed_rna_fasta, $run_dir."/".$Pipeline::preprocessed_rna_fasta.".stats", \@pre_rna_tags, "preprocessed_rna");
my @pre_tags = grep { $_ =~ /_preprocessed$/ } @$stat_tags;
load_fasta_stats($job_num, $results_dir."/".$Pipeline::preprocessed_fasta, $run_dir."/".$Pipeline::preprocessed_fasta.".stats", \@pre_tags, "preprocessed");

# load processed rna stats
my @proc_rna_tags = grep { $_ =~ /_processed_rna$/ } @$stat_tags;
my $proc_rna_file = (-s $results_dir."/".$Pipeline::processed_fasta_rna_1) ? $Pipeline::processed_fasta_rna_1 : $Pipeline::processed_fasta_rna_2;
load_fasta_stats($job_num, $results_dir."/".$proc_rna_file, $run_dir."/".$proc_rna_file.".stats", \@proc_rna_tags, "processed_rna");
load_fasta_stats($job_num, $results_dir."/".$Pipeline::cluster_map_rna, $run_dir."/".$Pipeline::cluster_map_rna.".stats", \@proc_rna_tags, "processed_rna");
my $rna_pred  = $run_dir."/".$Pipeline::processed_fasta_rna_2.".stats";
my $rna_reads = '0';
if (-s $rna_reads) {
  $rna_reads = `grep -w sequence_count $rna_pred | cut -f2`;
  chomp $rna_reads;
}
load_stats($job_num, [['read_count_processed_rna', $rna_reads]]);

# load processed aa stats
my @proc_aa_tags = grep { $_ =~ /_processed_aa$/ } @$stat_tags;
my $proc_aa_file = (-s $results_dir."/".$Pipeline::processed_fasta_aa_1) ? $Pipeline::processed_fasta_aa_1 : $Pipeline::processed_fasta_aa_2;
load_fasta_stats($job_num, $results_dir."/".$proc_aa_file, $run_dir."/".$proc_aa_file.".stats", \@proc_aa_tags, "processed_aa");
load_fasta_stats($job_num, $results_dir."/".$Pipeline::cluster_map_aa, $run_dir."/".$Pipeline::cluster_map_aa.".stats", \@proc_aa_tags, "processed_aa");
my $aa_pred  = $results_dir."/".$Pipeline::processed_fasta_aa_2;
my $aa_reads = '0';
if (-s $aa_pred) {
  $aa_reads = `grep '^>' $aa_pred | sed 's/_[0-9]*_[0-9]*_[-+]\$//' | sort -u | wc -l`;
  chomp $aa_reads;
}
load_stats($job_num, [['read_count_processed_aa', $aa_reads]]);

unless (!(-e "$results_dir/700.annotation.sims.stats") && -e "$results_dir/$stage_id.$stage_name.sims.stats") {
  move("$results_dir/700.annotation.sims.stats", "$results_dir/$stage_id.$stage_name.sims.stats") or fail($log, "$stage_name failed on job: $job_num, can not move $results_dir/700.annotation.sims.stats to $results_dir/$stage_id.$stage_name.sims.stats");
}
load_fasta_stats($job_num, $raw_fna, "$results_dir/$stage_id.$stage_name.sims.stats", ['sequence_count_sims_rna', 'sequence_count_sims_aa', 'sequence_count_ontology'], 0);

# annotated read stat
my $ann_read = '0';
if (-s "$results_dir/900.loadDB.sims.filter.seq") {
  $ann_read = `cut -f1 $results_dir/900.loadDB.sims.filter.seq | sort -u | wc -l`;
  chomp $ann_read;
}
load_stats($job_num, [['read_count_annotated', $ann_read]]);

# calculate taxa abundances
foreach my $taxa (('domain', 'phylum', 'class', 'order', 'family', 'genus', 'species')) {
  my $other = ($taxa eq 'domain') ? 1 : 0;
  my $taxa_stats = Pipeline::get_taxa_abundances($job_num, $taxa, $other, $ver_db);
  if (@$taxa_stats > 0) {
    open(TSTAT, ">$run_dir/$stage_id.$stage_name.$taxa.stats") or fail($log, "$stage_name failed on job: $job_num, can not open $run_dir/$stage_id.$stage_name.$taxa.stats: $!");
    print TSTAT join("\n", map { $_->[0] . "\t" . $_->[1] } @$taxa_stats) . "\n";
    close TSTAT;
  }
}

# calculate function abundances
my $func_stats = Pipeline::get_function_abundances($job_num, $ver_db);
if (@$func_stats > 0) {
  open(FSTAT, ">$run_dir/$stage_id.$stage_name.functions.stats") or fail($log, "$stage_name failed on job: $job_num, can not open $run_dir/$stage_id.$stage_name.functions.stats: $!");
  print FSTAT join("\n", map { $_->[0] . "\t" . $_->[1] } @$func_stats) . "\n";
  close FSTAT;
}

# calculate ontology abundance
my $ontol_stats = Pipeline::get_ontology_abundances($job_num, $ver_db);
foreach my $ontol (keys %$ontol_stats) {
  if (@{$ontol_stats->{$ontol}} > 0) {
    open(OSTAT, ">$run_dir/$stage_id.$stage_name.$ontol.stats") or fail($log, "$stage_name failed on job: $job_num, can not open $run_dir/$stage_id.$stage_name.$ontol.stats: $!");
    print OSTAT join("\n", map { $_->[0] . "\t" . $_->[1] } @{$ontol_stats->{$ontol}}) . "\n";
    close OSTAT;
  }
}

# calculate rarefaction coordanates
my $rare_stats = Pipeline::get_rarefaction_xy($job_num, $ver_db);
if (@$rare_stats > 0) {
  open(RSTAT, ">$run_dir/$stage_id.$stage_name.rarefaction.stats") or fail($log, "$stage_name failed on job: $job_num, can not open $run_dir/$stage_id.$stage_name.rarefaction.stats: $!");
  print RSTAT join("\n", map { $_->[0] . "\t" . $_->[1] } @$rare_stats) . "\n";
  close RSTAT;
}

# calculate alpha diversity
my $alpha = Pipeline::get_alpha_diversity($job_num, 'shannon', $ver_db);
load_stats($job_num, [['alpha_diversity_shannon', $alpha]]);

# calculate ratio identified reads
my $tag_names = ['sequence_count_sims_rna','cluster_count_processed_rna','clustered_sequence_count_processed_rna','sequence_count_preprocessed_rna',
		 'sequence_count_sims_aa','cluster_count_processed_aa','clustered_sequence_count_processed_aa','sequence_count_preprocessed'];
my $stat_set      = Pipeline::get_job_statistics($job_num, $stat_tags);
my $data_set      = Pipeline::get_job_attributes($job_num, ['sequence_type_guess']);
my $qc_aa_seqs    = exists($stat_set->{sequence_count_preprocessed}) ? $stat_set->{sequence_count_preprocessed} : 0;
my $aa_sims       = exists($stat_set->{sequence_count_sims_aa}) ? $stat_set->{sequence_count_sims_aa} : 0;
my $aa_clusts     = exists($stat_set->{cluster_count_processed_aa}) ? $stat_set->{cluster_count_processed_aa} : 0;
my $aa_clust_seq  = exists($stat_set->{clustered_sequence_count_processed_aa}) ? $stat_set->{clustered_sequence_count_processed_aa} : 0;
my $qc_rna_seqs   = exists($stat_set->{sequence_count_preprocessed_rna}) ? $stat_set->{sequence_count_preprocessed_rna} : 0;
my $rna_sims      = exists($stat_set->{sequence_count_sims_rna}) ? $stat_set->{sequence_count_sims_rna} : 0;
my $rna_clusts    = exists($stat_set->{cluster_count_processed_rna}) ? $stat_set->{cluster_count_processed_rna} : 0;
my $rna_clust_seq = exists($stat_set->{clustered_sequence_count_processed_rna}) ? $stat_set->{clustered_sequence_count_processed_rna} : 0;
my $aa_ratio      = $qc_aa_seqs ? ($aa_sims - $aa_clusts + $aa_clust_seq) / $qc_aa_seqs : 0;
my $rna_ratio     = $qc_rna_seqs ? ($rna_sims - $rna_clusts + $rna_clust_seq) / $qc_rna_seqs : 0;
load_stats($job_num, [['ratio_reads_aa', sprintf("%.3f", $aa_ratio)], ['ratio_reads_rna', sprintf("%.3f", $rna_ratio)]]);

# trust amplicon
my $seq_guess = exists($data_set->{sequence_type_guess}) ? $data_set->{sequence_type_guess} : '';
if ($seq_guess eq 'Amplicon') {
  Pipeline::set_jobcache_info($job_num, 'sequence_type', 'Amplicon');
}
# use ratio for WGS or MT
else {
  my $ratio_guess = ($rna_ratio > 0.25) ? 'MT' : 'WGS';
  Pipeline::set_jobcache_info($job_num, 'sequence_type', $ratio_guess);
}

# gzip files
system("compress_job -j $job_num") == 0 or fail($log, "compress_job: ".$message);

# copy output to somewhere
opendir(RUNDIR, $run_dir) or fail($log, "$stage_name failed on job: $job_num, can not open $run_dir: $!");
my @stats = grep { /\.(stats|lens|gcs)$/ } readdir(RUNDIR);
closedir RUNDIR;
foreach my $sf (@stats) {
  move("$run_dir/$sf", "$results_dir/$sf") or fail($log, "$stage_name failed on job: $job_num, can not move $sf");
  chmod 0666, "$results_dir/$sf";
}

# set job as viewable
Pipeline::set_jobcache_info($job_num, "viewable", 1);
my $job_info = Pipeline::get_jobcache_info($job_num);
$stat_set      = Pipeline::get_job_statistics($job_num);
$data_set      = Pipeline::get_job_attributes($job_num, ['sequencing_method_guess']);
my $project_name = Pipeline::get_job_project_name($job_num);

# create solr document for metagenome search
my $solr_fn = "$results_dir/999.done.solr.json";
my $func_fn = "$results_dir/$stage_id.$stage_name.functions.stats";
my $org_fn = "$results_dir/$stage_id.$stage_name.species.stats";
my $md5_fn = "$results_dir/900.loadDB.sims.filter.seq";

open SOLR, ">$solr_fn" or fail($log, "$stage_name failed on job: $job_num, can not write file $solr_fn");
print SOLR "[\n{\n";
print SOLR "   \"job\" : \"$job_num\",\n";
print SOLR "   \"id\" : \"mgm".$job_info->{metagenome_id}."\",\n";
print SOLR "   \"id_sort\" : \"mgm".$job_info->{metagenome_id}."\",\n";
print SOLR "   \"status\" : \"private\",\n";
print SOLR "   \"status_sort\" : \"private\",\n";
my $dt = DateTime->now;
print SOLR "   \"created\" : \"$dt"."Z\",\n";
print SOLR "   \"name\" : \"".$job_info->{name}."\",\n";
print SOLR "   \"name_sort\" : \"".$job_info->{name}."\",\n";
print SOLR "   \"project_id\" : \"".$job_info->{primary_project}."\",\n";
print SOLR "   \"project_id_sort\" : \"".$job_info->{primary_project}."\",\n";
print SOLR "   \"project_name\" : \"$project_name\",\n";
print SOLR "   \"project_name_sort\" : \"$project_name\",\n";
print SOLR "   \"sequence_type\" : \"".$job_info->{sequence_type}."\",\n";
print SOLR "   \"seq_method\" : \"".$data_set->{sequencing_method_guess}."\",\n";
print SOLR "   \"version\" : 1,\n";

foreach my $stat (keys %$stat_set) {
  if($stat =~ /count/ || $stat =~ /min/ || $stat =~ /max/) {
    print SOLR "   \"$stat\_l\" : \"".$stat_set->{$stat}."\",\n";
  } else {
    print SOLR "   \"$stat\_d\" : \"".$stat_set->{$stat}."\",\n";
  }
}

print SOLR "   \"function\" : [\n";
my $line_count = 0;
open FUNCS, "$func_fn" or fail($log, "$stage_name failed on job: $job_num, can not read file $func_fn");
while(my $line=<FUNCS>) {
  chomp $line;
  $line_count++;
  my @array = split(/\t/, $line);
  $array[0] =~ s/\"/\\"/g;
  if($line_count == 1) {
    print SOLR "      \"$array[0]\"";
  } else {
    print SOLR ",\n      \"$array[0]\"";
  }
}
close FUNCS;

print SOLR "\n   ],\n   \"organism\" : [\n";
$line_count = 0;
open ORGS, "$org_fn" or fail($log, "$stage_name failed on job: $job_num, can not read file $org_fn");
while(my $line=<ORGS>) {
  chomp $line;
  $line_count++;
  my @array = split(/\t/, $line);
  $array[0] =~ s/\"/\\"/g;
  if($line_count == 1) {
    print SOLR "      \"$array[0]\"";
  } else {
    print SOLR ",\n      \"$array[0]\"";
  }
}
close FUNCS;

print SOLR "\n   ],\n   \"md5\" : [\n";
my $last_md5 = "";
$line_count = 0;
open MD5S, "$md5_fn" or fail($log, "$stage_name failed on job: $job_num, can not read file $md5_fn");
while(my $line=<MD5S>) {
  chomp $line;
  $line_count++;
  my @array = split(/\t/, $line);
  if($line_count == 1) {
    print SOLR "      \"$array[1]\"";
  } elsif($array[1] ne $last_md5) {
    print SOLR ",\n      \"$array[1]\"";
  }
  $last_md5 = $array[1];
}
print SOLR "\n   ]\n}\n]";
close SOLR;

my $post_url = "http://$Pipeline_Conf::job_solr/solr/$Pipeline_Conf::job_collect/update/json?commit=true";
my $req = HTTP::Request::StreamingUpload->new(
  POST    => $post_url,
  path    => $solr_fn,
  headers => HTTP::Headers->new(
    'Content-Type'   => 'application/json',
    'Content-Length' => -s $solr_fn,
  ),
);

my $response = LWP::UserAgent->new->request($req);
if($response->{"_msg"} ne 'OK') {
  my $content = $response->{"_content"};
  fail($log, "$stage_name failed on job: $job_num, received error and response: \"$content\" when POSTing file \"$solr_fn\" to \"$post_url\".");
}

# email owner on completion
if ($email) {
  my $mailer   = Mail::Mailer->new();
  my $owner    = Pipeline::get_job_owner($job_num);
  my $name     = $owner->{firstname} . " " . $owner->{lastname};
  my $body_txt = "Dear $name,\n\nThe annotation job that you submitted for '" . $job_info->{name} . "' has completed.\n\n" .
                 "Your job has been assigned MG-RAST metagenome ID " . $job_info->{metagenome_id} .
		 " and can be linked to using:\n$mg_link" . $job_info->{metagenome_id} . "\n\n" .
		 'This is an automated message.  Please contact mg-rast@mcs.anl.gov if you have any questions or concerns.';

  $mailer->open({ From    => $mg_email,
		  To      => "$name <" . $owner->{email} . ">",
		  Subject => "MG-RAST Job Completed",
		})
    or die "Can't open Mail::Mailer: $!\n";
  print $mailer $body_txt;
  $mailer->close();
}

# last task should be to remove nodes (files) from Shock
unless(-s "$results_dir/800.download_from_awe.awf" != 0) {
  fail($log, "$stage_name failed on job: $job_num, AWE workflow document is empty or size zero.");
}
open AWE_JOB, "$results_dir/800.download_from_awe.awf" || fail($log, "$stage_name failed on job: $job_num, cannot open AWE workflow document.");
my $line=<AWE_JOB>;
close AWE_JOB;
chomp $line;
my $json = from_json( $line, { utf8 => 1 } );
my %shock_nodes = ();
foreach my $task (@{$json->{data}->{tasks}}) {
  foreach my $input (keys %{$task->{inputs}}) {
    my $node = $task->{inputs}->{$input}->{node};
    $shock_nodes{$node} = 1;
  }
  foreach my $output (keys %{$task->{outputs}}) {
    my $node = $task->{outputs}->{$output}->{node};
    $shock_nodes{$node} = 1;
  }
}

$log->info("Removing nodes from Shock");
my $ua = LWP::UserAgent->new();
foreach my $node (keys %shock_nodes) {
  my $delete = $ua->delete("http://".$Pipeline_Conf::shock_url_from_mgrast."/node/$node",
                           Authorization => "OAuth $Pipeline_Conf::awe_pipeline_token");
  if($delete->{'_msg'} eq 'OK') {
    $log->info("Deleted node $node from Shock.");
  } else {
    $log->info("Could not delete node $node from Shock, msg returned was: ".$delete->{'_msg'});
  }
}

$log->info("Finished $stage_name on job: $job_num");

# update jobcache stage status
Pipeline::update_stage_info($job_num, $stage_name, "completed");

exit(0);

sub fail {
  my ($log, $message) = @_;
  Pipeline::update_stage_info($job_num, $stage_name, "error");
  $log->error($message);
  exit(1);
}

sub load_fasta_stats {
  my ($job_num, $fasta_file, $stats_file, $tags, $step) = @_;

  unless (-s $fasta_file) {
    $log->info("Skipping loading stats for stage '$step', $fasta_file is empty");
    return;
  }
  unless (-s $stats_file) {
    fail($log, "$stage_name failed on job: $job_num, missing file: $stats_file");
  }
  my @stats = `cat $stats_file`;
  chomp @stats;
  if ( $stats[0] =~ /^ERROR/i ) {
    fail($log, "$stage_name failed on job: $job_num, bad stats $stats_file: $stats[0]");
  }

  my @data = ();
  my %tmap = map { $_, 1 } @$tags;
  foreach my $set (@stats) {
    my ($tag, $val) = split(/\t/, $set);
    my $jtag = $step ? $tag.'_'.$step : $tag;
    if (exists $tmap{$jtag}) {
      push @data, [$jtag, $val];
    }
  }
  load_stats($job_num, \@data);
}

sub load_stats {
  my ($job_num, $stats) = @_;
  my $res = Pipeline::set_job_statistics($job_num, $stats);
  unless ($res) {
    fail($log, "$stage_name failed on job: $job_num, loading JobDB stats");
  }  
}
