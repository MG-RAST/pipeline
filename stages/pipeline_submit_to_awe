#!/usr/bin/env perl 

#MG-RAST pipeline job submitter for AWE
#Command name: pipeline_submit_to_awe
#Options:
#     -job_num=<job number, required>
#     -upload=<input file that is local and to be uploaded, required>
#     -pipeline=<path for pipeline job template, required>
#     -awe=<AWE server URL (ip:port), required>
#     -shock=<Shock URL (ip:port), required>
#     -mem_host=<memcache URL (ip:port), required>
#     -name=<job name (default='default')>
#     -user=<user name (default='mgrastprod')>
#     -project=<project name (default='pipeline')>
#     -cgroups=<exclusive_client_group_list (separate by ',') (default='')>
#     -totalwork_sm=<number of workunits to split for splitable tasks of small jobs (default 8)>
#     -totalwork_lg=<number of workunits to split for splitable tasks of large jobs (default 16)>
#     -lg_size=<cutoff for size in bytes that categorizes a job as "large" (default 2147483648 bytes, aka 2GB)>
#     -bowtie=<boolean, if bowtie should be run (default 1)>
#     -dereplicate=<boolean, if dereplication should be performed (default 1)>
#     -filter_options=<default='filter_ambig:max_ambig=5:filter_ln:min_ln=1:max_ln=806'>
#     -screen_indexes=<default='h_sapiens_asm'>
#     
#Use case: submit a job with a local input file and a pipeline template (input file is local and will be uploaded to shock automatially;
#      Required options: -upload, -pipeline (if awe_pipeline_template not in Pipeline_Conf.pm), -awe (if awe_url not in Pipeline_Conf.pm), -shock (if shock_url not in Pipeline_Conf.pm)
#      Optional options: -name, -user, -project, -cgroups, -totalwork_sm, -totalwork_lg, -lg_size, -bowtie, -dereplicate, -filter_options, -screen_indexes
#      Operations:
#               1. upload input file to shock
#               2. create job script based on job template and available info
#               3. submit the job json script to awe
#               
#Note: if awe_pipeline_template, awe_url (ip:port), shock_url (ip:port), and mem_host_url (ip:port) are configured in Pipeline_Conf.pm, -pipeline, -awe, -shock, and -mem_host are not needed respectively. But the specified -pipeline, -awe,  and -shock will overwrite the Pipeline_Conf.pm variables.

use strict;
use warnings;
no warnings('once');

use Pipeline;
use Pipeline_Conf;

use DBI;
use JSON;
use Digest::MD5::File qw( file_md5_hex );
use File::Slurp;
use Getopt::Long;
use LWP::UserAgent;
use String::Random;
use Data::Dumper;
umask 000;

my $stage_name = "submit_to_awe";
my $stage;
for my $s (@{$Pipeline_Conf::pipeline->{'awe_part_one'}}){
  $stage = $s if $s->{name} eq $stage_name;
}
my $stage_id = $stage->{id};

# options
my $job_num = "";
my %vars = ();  # Hash to store variables that we'll replace in the AWE job template
$vars{inputfile} = "";
my $awe_url = "";
$vars{shockurl} = "";
$vars{mem_host} = "";
my $pipeline_template = "";
$vars{jobname} ="default";
$vars{user} = "mgrastprod";
$vars{project} = "pipeline";
$vars{clientgroups} = "";
my $totalwork_sm = 8;
my $totalwork_lg = 16;
my $lg_size = 2147483648;
$vars{bowtie} = 1;
$vars{dereplicate} = 1;
$vars{filter_options} = "filter_ambig:max_ambig=5:filter_ln:min_ln=1:max_ln=806";
$vars{screen_indexes} = "h_sapiens_asm";

my $help = 0;

my $options = GetOptions ("job_num=i" => \$job_num,
                          "upload=s"   => \$vars{inputfile},
			  "pipeline=s" => \$pipeline_template,
                          "awe=s"    => \$awe_url,
 			  "shock=s"  => \$vars{shockurl},
                          "mem_host=s" => \$vars{mem_host},
                          "name=s" => \$vars{jobname},
                          "user=s"   => \$vars{user},
                          "project=s" => \$vars{project},
                          "cgroups=s" => \$vars{clientgroups},
                          "totalwork_sm=i" => \$totalwork_sm,
                          "totalwork_lg=i" => \$totalwork_lg,
                          "lg_size=i" => \$lg_size,
                          "bowtie=i" => \$vars{bowtie},
                          "dereplicate=i" => \$vars{dereplicate},
                          "filter_options=s" => \$vars{filter_options},
                          "screen_indexes=s" => \$vars{screen_indexes},
                          "h"  => \$help,
			 );

$vars{aa_pid} = 90;
$vars{ach_annotation_ver} = length($Pipeline_Conf::ach_annotation_ver) > 0 ? $Pipeline_Conf::ach_annotation_ver : 1;
$vars{fgs_type} = 454;
$vars{md5rna_clust} = length($Pipeline_Conf::md5rna_clust) > 0 ? $Pipeline_Conf::md5rna_clust : "md5nr.clust";
$vars{md5rna_clust} =~ s/.*\/(.*)/$1/;
$vars{prefix_length} = 50;
$vars{rna_pid} = 97;

if ($help) {
    print_usage();
    exit 0;
}

if(length($job_num)==0) {
    print STDERR "$stage_name failed, job_num was not specified.\n";
    print_usage();
    exit 1;
}

my $log = Pipeline::logger($job_num);
$log->info("Starting $stage_name on job: $job_num");

if (length($vars{inputfile})==0) {
    $log->error("$stage_name failed on job: $job_num, please specify the local path of the input file.");
    print_usage();
    exit 1;
} elsif (! -e $vars{inputfile}) {
    $log->error("$stage_name failed on job: $job_num, the input file [".$vars{inputfile}."] does not exist.");
    print_usage();
    exit 1;  
}

if (length($pipeline_template)==0) {
    $pipeline_template = $Pipeline_Conf::awe_pipeline_template;
    if (length($pipeline_template)==0) {
        $log->error("$stage_name failed on job: $job_num, a pipeline template was not specified.");
        print_usage();
        exit 1;  
    } elsif (! -e $pipeline_template) {
        $log->error("$stage_name failed on job: $job_num, the pipeline template file [$pipeline_template] does not exist.");
        print_usage();
        exit 1;
    }
}
    
if (length($awe_url)==0) {
    $awe_url = $Pipeline_Conf::awe_url;
    if (length($awe_url)==0) {
        $log->error("$stage_name failed on job: $job_num, the AWE server URL was not specified.");
        print_usage();
        exit 1;
    }
}

if (length($vars{shockurl})==0 ) {
    $vars{shockurl} = $Pipeline_Conf::shock_url;
    if (length($vars{shockurl})==0) {
        $log->error("$stage_name failed on job: $job_num, the Shock server URL was not specified.");
	print_usage();
	exit 1;
    }
}

if (length($vars{mem_host})==0 ) {
    $vars{mem_host} = $Pipeline_Conf::mem_host_url;
    if (length($vars{mem_host})==0) {
        $log->error("$stage_name failed on job: $job_num, the memcache server URL was not specified.");
	print_usage();
	exit 1;
    }
}

my $job_dir   = $Pipeline_Conf::global_job_dir."/".$job_num;
my $proc_dir  = $job_dir."/proc";
my $stage_dir = $proc_dir."/".$stage_id.".".$stage_name;
mkdir($stage_dir) or fail($log, "Could not mkdir: $stage_dir, $!");

#upload input to shock
$log->info("Uploading input file: ".$vars{inputfile}." to Shock");

my $md5 = file_md5_hex( $vars{inputfile} );
my $ua = LWP::UserAgent->new();
my $post = $ua->post("http://".$vars{shockurl}."/node",
                     Content_Type => 'form-data',
                     Content      => [ upload => [$vars{inputfile}] ]
                    );

my $json = new JSON();
my $res = $json->decode( $post->content );
my $res_md5 = $res->{data}->{file}->{checksum}->{md5};
$vars{shocknode} = $res->{data}->{id};

if($md5 ne $res_md5) {
    $log->error("$stage_name failed on job: $job_num, Shock upload of input file to Shock node:".$vars{shocknode}." was not successful.");
    exit(1);
}

$log->info("Upload of input file: ".$vars{inputfile}." complete. Shock node id=".$vars{shocknode}."\n");

#generate job script based on template (instantiate a job script with availalbe information filled into the template)
my $pipeline_name = $pipeline_template;
$pipeline_name =~ s/(.*)\.template/$1/;
if (length($vars{jobname})==0) {
    $vars{jobname} = $pipeline_name.".job";
}

# setting totalwork variable
my $file_size = -s $vars{inputfile};
if($file_size >= $lg_size) {
    $vars{totalwork} = $totalwork_lg;
} else {
    $vars{totalwork} = $totalwork_sm;
}

# inputfile in awe workflow cannot contain directory path, only filename, so removing path here.
$vars{inputfile} =~ s/.*\/(.*)/$1/;

# Replace # vars in template
my $text = read_file($pipeline_template);
foreach my $key (keys %vars) {
    $text =~ s/#$key/$vars{$key}/g;
}

my $jobscript = "$stage_dir/awe_workflow.$job_num.json";
open OUT, ">$jobscript" || fail($log, "Could not open $jobscript for writing.");
print OUT $text;
close OUT;

#upload job script to awe server
$log->info("Submitting job script ($jobscript) to AWE.\n");
$ua = LWP::UserAgent->new();
$post = $ua->post("http://".$awe_url."/job",
                  Content_Type => 'form-data',
                  Content      => [ upload => [$jobscript] ]
                 );

$res = $json->decode( $post->content );

my $awe_id = $res->{data}->{id};
my $job_id = $res->{data}->{jid};
my $state = $res->{data}->{state};

my $awe_dump_file = "$stage_dir/awe_submission_output.txt";

open OUT, ">$awe_dump_file" || fail($log, "Could not open $awe_dump_file for writing.");
print OUT Dumper($res);
close OUT;

if($state ne "submitted") {
    $log->error("$stage_name failed on job: AWE job submission was not successful, please see '$awe_dump_file' for more info.");
    exit(1);
}

print " Done! awe_id=".$awe_id." job_id=$job_id\n";
$log->info("AWE submission complete. AWE URL = http://".$awe_url."/job/".$awe_id." Shock URL = http://".$vars{shockurl}."/node/".$vars{shocknode});

$log->info("Adding job to AWE MG-RAST lookup database.");
my $db = $Pipeline_Conf::awe_mgrast_lookup_db;
my $host = $Pipeline_Conf::awe_mgrast_lookup_host;
my $user = $Pipeline_Conf::awe_mgrast_lookup_user;
my $pass = $Pipeline_Conf::awe_mgrast_lookup_pass;
my $table = $Pipeline_Conf::awe_mgrast_lookup_table;
my $dbh = DBI->connect("DBI:mysql:$db;host=$host", $user, $pass) || fail($log, "Couldn't connect to database: ".DBI->errstr);
my $str = "INSERT INTO $table (job, awe_id, status, submitted, last_update) VALUES($job_num, '$awe_id', '$state', now(), now())";
my $sth = $dbh->prepare($str) || fail($log, "Couldn't prepare statement: " . $dbh->errstr);
$sth->execute() || fail($log, "Couldn't execute statement: " . $sth->errstr);

$log->info("Job added to AWE MG-RAST lookup database.");
$log->info("Finished $stage_name on job: $job_num");

exit(0);

sub fail {
    my ($log, $message) = @_;
    Pipeline::update_stage_info($job_num, $stage_name, "error");
    $log->error($message);
    exit(1);
}

sub print_usage{
    print "
MG-RAST pipeline job submitter for AWE
Command name: pipeline_submit_to_awe
Options:
     -job_num=<job number, required>
     -upload=<input file that is local and to be uploaded, required>
     -pipeline=<path for pipeline job template, required>
     -awe=<AWE server URL (ip:port), required>
     -shock=<Shock URL (ip:port), required>
     -mem_host=<memcache URL (ip:port), required>
     -name=<job name (default='default')>
     -user=<user name (default='mgrastprod')>
     -project=<project name (default='pipeline')>
     -cgroups=<exclusive_client_group_list (separate by ',') (default='')>
     -totalwork=<number of workunits to split for splitable tasks (default 1)>
     -bowtie=<boolean, if bowtie should be run (default 1)>
     -dereplicate=<boolean, if dereplication should be performed (default 1)>
     -filter_options=<default='filter_ambig:max_ambig=5:filter_ln:min_ln=1:max_ln=806'>
     -screen_indexes=<default='h_sapiens_asm'>
     
Use case: submit a job with a local input file and a pipeline template (input file is local and will be uploaded to shock automatially;
      Required options: -upload, -pipeline (if awe_pipeline_template not in Pipeline_Conf.pm), -awe (if awe_url not in Pipeline_Conf.pm), -shock (if shock_url not in Pipeline_Conf.pm)
      Optional options: -name, -user, -project, -cgroups, -totalwork, -bowtie, -dereplicate, -filter_options, -screen_indexes
      Operations:
               1. upload input file to shock
               2. create job script based on job template and available info
               3. submit the job json script to awe
               
Note: if awe_pipeline_template, awe_url (ip:port), shock_url (ip:port), and mem_host_url (ip:port) are configured in Pipeline_Conf.pm, -pipeline, -awe, -shock, and -mem_host are not needed respectively. But the specified -pipeline, -awe,  and -shock will overwrite the Pipeline_Conf.pm variables.\n";
}
